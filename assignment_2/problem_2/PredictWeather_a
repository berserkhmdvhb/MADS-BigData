////imports////

import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.tree._
import org.apache.spark.mllib.tree.model._
import org.apache.spark.rdd._
import spark.implicits._
import org.apache.spark.sql.types._
import org.apache.spark.mllib.evaluation._
import org.apache.spark.ml.linalg.{Vector, Vectors}
import org.apache.spark.ml.param.ParamMap
import org.apache.spark.sql.Row
import org.apache.spark.sql.SparkSession
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.feature.{VectorAssembler, FeatureHasher, IndexToString, StringIndexer, VectorIndexer}
import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.rdd._
import org.apache.spark.ml.regression.{RandomForestRegressionModel, RandomForestRegressor}
import org.apache.spark.ml.evaluation.RegressionEvaluator
// hyperparams import
import org.apache.spark.sql.Row
import org.apache.spark.ml.tuning.{CrossValidator,ParamGridBuilder}
import org.apache.spark.ml.stat.Correlation
import org.apache.spark.ml.feature.QuantileDiscretizer
import org.apache.spark.mllib.stat.Statistics


//start timing the code
val start = System.nanoTime()


// for reading the dataframe

val sqlContext = new org.apache.spark.sql.SQLContext(sc)
/////////PHASE 0: PREPARATION////////////
// define a parse function that maps the rrd file so that from each line it creates variables based on position of contents in the file
// After creating the variables (year, longitude , etc), the function put them together inside
// an array and output the rdd created from array
def parse(rawData: RDD[String]): RDD[Array[Double]] = {
  rawData
    .filter(line => line.substring(87, 92) != "+9999") // filter out missing temperature labels
    .map { line =>
      val year = line.substring(15, 19).toInt
      val month = line.substring(19, 21).toInt
      val day = line.substring(21, 23).toInt
      val hour = line.substring(23, 25).toInt
      val latitude = line.substring(28, 34).toDouble / 1000
      val longitude = line.substring(34, 41).toDouble / 1000
      val elevationDimension = line.substring(46, 51).toInt
      val directionAngle = line.substring(60, 63).toInt
      val speedRate = line.substring(65, 69).toDouble / 10
      val ceilingHeightDimension = line.substring(70, 75).toInt
      val distanceDimension = line.substring(78, 84).toInt
      val dewPointTemperature = line.substring(93, 98).toDouble / 10
      val airTemperature = line.substring(87, 92).toDouble / 10
      Array(year, month, day, hour, latitude, longitude, elevationDimension, directionAngle, speedRate, ceilingHeightDimension, distanceDimension, dewPointTemperature, airTemperature)
    }
}


// apply the parse function on data
val rawNOAA = sc.textFile("/home/users/hvaheb/datasets/NOAA-065900/065900*")
val parsedNOAA = parse(rawNOAA)

// convert rdd to dataframe, which gives us one column with rows which are stacked (wrapped) array that are
// obtained from the parsing function
val df_raw = parsedNOAA.toDF()

// now we unstack the array by creating meaningful columns with corresponding datatype
val df_temp = df_raw.withColumn("year", $"value".getItem(0).cast(IntegerType)).withColumn("month", $"value".getItem(1).cast(IntegerType)).withColumn("day", $"value".getItem(2).cast(IntegerType)).withColumn("hour", $"value".getItem(3).cast(IntegerType)).withColumn("latitude", $"value".getItem(4)).withColumn("longitude", $"value".getItem(5)).withColumn("elevationDimension", $"value".getItem(6).cast(IntegerType)).withColumn("directionAngle", $"value".getItem(7).cast(IntegerType)).withColumn("speedRate", $"value".getItem(8)).withColumn("ceilingHeightDimension", $"value".getItem(9).cast(IntegerType)).withColumn("distanceDimension", $"value".getItem(10).cast(IntegerType)).withColumn("dewPointTemperature", $"value".getItem(11)).withColumn("airTemperature", $"value".getItem(12))

val unwantedColumns= df_temp.columns.filter (x => x == "value")
val df = df_temp.drop(unwantedColumns: _*).withColumnRenamed("airTemperature","label")

// save dataframe to file
df.write.option("header",true).csv("/home/users/hvaheb/bds/bda/exercises/2/out/data")
//val sampleSize = 0.01 // use 1 percent sample size for debugging!
//val df = df_temp2.sample(false, sampleSize)
//size and outline of dataframe
df.count()
df.show()



///////PHASE I: TRANSFORMERS and ESTIMATORS////////

///1.1 TRANSFORMERS////


// filter feature variables
val features = df.columns.filter(! _.contains("label"))



// VectorAssembler is a transformer that combines a given list of columns into a single vector column
val assembler = new VectorAssembler().setInputCols(features).setOutputCol("features")







//1.2 ESTIMATORS////


// split dataframe to train and test
val condition =  col("year") >= 1949 && col("year") <= 2021
val condition2 =  col("year") == 2022
val trainData = df.filter(condition)
val testData = df.filter(not(condition))
// cache() is recommended to prevent repeating the calculation
trainData.cache()
testData.cache()

// define the model
val rf = new RandomForestRegressor().setLabelCol("label").setFeaturesCol("features").setFeatureSubsetStrategy("auto")
///////PHASE II: Train, Test, Evaluate////

// create a pipeline comprising elements form Phase I, i.e.,
// chain transformers(indexers) and estimators(DecisionTreeClassifier) in a Pipeline
val stages = Array(assembler, rf)
val pipeline = new Pipeline().setStages(stages)


val models = pipeline.fit(trainData)
val predictions = models.transform(testData)

// print logic and decision structure of random forest
//val rfModel = model.stages(1).asInstanceOf[RandomForestRegressionModel]
//println(s"Learned regression forest model:\n ${rfModel.toDebugString}"


///////PHASE III: Hyper-parameter Tuning////
/// create the last element of pipeline, which is a 5-fold cross-validation on a
/// hyperparameter grid space
val evaluatorReg = new RegressionEvaluator()
val paramGrid = new ParamGridBuilder().addGrid(rf.maxBins, 100 :: 200 :: Nil).addGrid(rf.maxDepth, 5 :: 10 :: Nil).build()
val cv = new CrossValidator().setEstimator(pipeline).setEstimatorParamMaps(paramGrid).setEvaluator(evaluatorReg).setNumFolds(5)



//fit the cross-validation model
val cvModel = cv.fit(trainData)

//calculate runtime
val stop = System.nanoTime()
val diff_in_s = (stop - start) / 1e9d
val secs = (diff_in_s % 60).toInt
val mins = ((diff_in_s / 60.0) % 60).toInt
Console.println(s"Elapsed time: ${mins} min ${secs} s")


// make sure that we have the desired number of folds
cvModel.getNumFolds


// make predictions on test data. cvModel uses the best model found
val predictionsCV = cvModel.transform(testData)

//remove unwanted columns from prediction
val unwantedColumnsCV = predictionsCV.columns.filter (x => x != "label" & x != "indexedFeatures" & x!= "prediction" & x!= "rawPrediction" & x != "probability")
val predictionsCVClean = predictionsCV.drop(unwantedColumnsCV: _*)


//save the best model on the specified path
val path = "/home/users/hvaheb/bds/bda/exercises/2/out/"
val modelPath = path + "/model_rfr"
cvModel.write.overwrite().save(path=modelPath)


// report RMSE

// Select (prediction, true label) and compute test error using RMSE metrics.
val evaluator = new RegressionEvaluator().setLabelCol("label").setPredictionCol("prediction").setMetricName("rmse")
val rmse = evaluator.evaluate(predictionsCVClean)
println(s"Root Mean Squared Error (RMSE) on test data = $rmse")


